{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.signal import clean\n",
    "from numpy import pi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cross_decomposition import PLSCanonical\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "PATH = '...'  # Path to the data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_divergent(X, cmap):\n",
    "    colors_div = sns.color_palette(cmap, n_colors=100)\n",
    "    X_min = np.min(X)\n",
    "    X_max = np.max(X)\n",
    "    X_scaled = ((X-X_min)/(X_max-X_min)*99).astype(int)\n",
    "    clrs = []\n",
    "    for c in X_scaled:\n",
    "        clrs.append(colors_div[c])\n",
    "    return clrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGroupedColorFunc(object):\n",
    "    def __init__(self, color_to_words, default_color):\n",
    "        self.word_to_color = {word: color\n",
    "                              for (color, words) in color_to_words.items()\n",
    "                              for word in words}\n",
    "\n",
    "        self.default_color = default_color\n",
    "\n",
    "    def __call__(self, word, **kwargs):\n",
    "        return self.word_to_color.get(word, self.default_color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_plot(df, colors, mask=None, ylabel=None,\n",
    "                   ylim=None, title=None, export_figures=False,\n",
    "                   fig_name=None):\n",
    "    # Find FDR Thres\n",
    "    n_t = df.shape[0]\n",
    "    clrs = []\n",
    "    for c, cc in enumerate(df.phen_cat_num):\n",
    "        clrs.append(colors[df.phen_cat_num[c]])\n",
    "    sns.set_theme(style=\"ticks\", palette=\"bright\", font_scale=1.2,\n",
    "                  font='Helvetica Neue', rc={\"axes.spines.right\": False,\n",
    "                                             \"axes.spines.top\": False,\n",
    "                                             \"axes.spines.left\": False})\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 6))\n",
    "    plot = sns.scatterplot(data=df, x='index', y='beta',\n",
    "                           hue='phen_cat_num', palette=colors,\n",
    "                           linewidth=0.1, edgecolor='w', legend=False)\n",
    "    # Find center of each category\n",
    "    t_df = df.groupby('phen_cat_num')['index'].median()\n",
    "    # Find maximum of each category\n",
    "    t_dfm = np.array(df.groupby('phen_cat_num')['index'].max())  # uncomment\n",
    "\n",
    "    # Axis labels\n",
    "    if ylabel:\n",
    "        plot.set_ylabel(ylabel)\n",
    "    else:\n",
    "        plot.set_ylabel('PLS coefficient')\n",
    "    plot.set_xlabel('')\n",
    "    # xticks to categories\n",
    "    plot.set_xticks(t_df)\n",
    "    plt.gca().set_xticklabels(df.phen_cat.drop_duplicates().values,\n",
    "                              rotation=60, ha='right', fontsize=14)\n",
    "    # Color sorted xticks\n",
    "    for xtick, color in zip(sorted(plt.gca().get_xticklabels(), key=lambda x: float(x.get_position()[0])), colors):\n",
    "        xtick.set_color(color)\n",
    "    if ylim:\n",
    "        plot.set_ylim(ylim)\n",
    "    if title:\n",
    "        plot.set_title(title)\n",
    "    plt.tick_params(axis='x', bottom=False)\n",
    "    [plt.axvline(x=xc, color='grey', linestyle='--', linewidth=0.5)\n",
    "     for xc in t_dfm]\n",
    "    plt.xlim(left=-20, right=len(df.index) + 20)\n",
    "    plt.axhline(y=0, linewidth=2, color='k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radar_barplot(data=None, title=\"\", axis_range=None, label=None, color=None, width=0.25):\n",
    "    # Create dataframe\n",
    "    class_mean = pd.DataFrame(data=data.T, index=label, columns=['mean']).T\n",
    "    # Number of variable\n",
    "    categories = list(class_mean)\n",
    "    N = len(categories)\n",
    "    # We are going to plot the first line of the data frame.\n",
    "    # But we need to repeat the first value to close the circular graph:\n",
    "    values = class_mean.values.flatten().tolist()\n",
    "    # Angle? (we divide the plot / number of variable)\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    # Initialise the spider plot\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    ax.spines[\"polar\"].set_visible(False)\n",
    "    # Draw one axe per variable + add labels labels yet\n",
    "    plt.xticks(angles, label, color=\"black\", size=16)\n",
    "    ax.xaxis.get_majorticklabels()[0].set_horizontalalignment(\"left\")\n",
    "    ax.xaxis.get_majorticklabels()[2].set_verticalalignment(\"bottom\")\n",
    "    ax.xaxis.get_majorticklabels()[3].set_verticalalignment(\"top\")\n",
    "    # Draw ylabels\n",
    "    ax.set_rlabel_position(0)\n",
    "    if axis_range is None:\n",
    "        axis_range = (np.min(values), np.max(values))\n",
    "    inc = (axis_range[1] - axis_range[0]) / 4\n",
    "    newinc = [\n",
    "        axis_range[0] + inc,\n",
    "        axis_range[0] + (inc * 2),\n",
    "        axis_range[0] + (inc * 3),\n",
    "        0,\n",
    "    ]\n",
    "    plt.yticks(\n",
    "        newinc, [str(\"{:.2f}\".format(elem)) for elem in newinc],\n",
    "        color=\"black\", size=12\n",
    "    )\n",
    "    plt.ylim(axis_range)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    # Plot data\n",
    "    ax.bar(angles, np.abs(values), alpha=1,\n",
    "           width=width, linewidth=1, edgecolor='k',\n",
    "           color=color,\n",
    "           )\n",
    "    ax.yaxis.zorder = 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load genetic data\n",
    "df_cnv = pd.read_csv(PATH + '....csv', low_memory=False)  # Load CNV data\n",
    "df_cnv = df_cnv[df_cnv['TYPE'] == 'CTRL']  # Filter for CTRL type\n",
    "idx_cnv = df_cnv.SampleID.values  # Get sample IDs\n",
    "df_cnv.set_index('SampleID', inplace=True)  # Set SampleID as index\n",
    "df_cnv = df_cnv.loc[:, ['TYPE', 'sum_loeuf_inv', 'n_genes', 'gene_id']]  # Select relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean data\n",
    "df_brain = pd.read_csv(PATH + '....csv', index_col=0)  # Load structural data\n",
    "df_phens = pd.read_csv(PATH + '....csv', index_col=0)  # Load phenotypic data\n",
    "df_cov = pd.read_csv(PATH + '....csv', index_col=0)  # Load covariate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset to common subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_str = df_brain.index\n",
    "idx_phens = df_phens.index\n",
    "idx_genetic = df_cnv.index\n",
    "idx_cov = df_cov.index\n",
    "\n",
    "idx_all = list(set(idx_str) & set(idx_phens) & set(idx_genetic) & set(idx_cov))\n",
    "\n",
    "df_cnv = df_cnv.loc[idx_all, :]\n",
    "df_brain = df_brain.loc[idx_all, :]\n",
    "df_phens = df_phens.loc[idx_all, :]\n",
    "df_cov = df_cov.loc[idx_all, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean data for PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress out covariates from the brain data\n",
    "df_brain[:] = clean(df_brain.values, confounds=df_cov.loc[:, ['interview_age', 'sex', 'volume', 'scanner']].values,\n",
    "                    detrend=False, standardize=False, standardize_confounds=True, extrapolate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress out covariates from the phenotypic data\n",
    "df_phens[:] = clean(df_phens.values, confounds=df_cov.loc[:, ['interview_age', 'sex']].values,\n",
    "                    detrend=False, standardize=False, standardize_confounds=True, extrapolate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score data\n",
    "df_phens[:] = scaler.fit_transform(df_phens.values)\n",
    "df_brain[:] = scaler.fit_transform(df_brain.values)\n",
    "x_ctrl_ss = df_brain.values.astype('float')\n",
    "y_ctrl_ss = df_phens.values.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce data with PCA\n",
    "pca_x = PCA(n_components=50)\n",
    "pca_y = PCA(n_components=50)\n",
    "\n",
    "x_ctrl_pca = pca_x.fit_transform(x_ctrl_ss)\n",
    "y_ctrl_pca = pca_y.fit_transform(y_ctrl_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLS machinery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PLS model\n",
    "ndim = 3  # Number of components\n",
    "pls = PLSCanonical(n_components=ndim, scale=False, max_iter=10000)  # Initialize PLS model\n",
    "pls.fit(x_ctrl_pca, y_ctrl_pca)  # Fit PLS model\n",
    "\n",
    "# PLS parameters\n",
    "x_rotations = pls.x_rotations_  # Get x rotations\n",
    "y_rotations = pls.y_rotations_  # Get y rotations\n",
    "\n",
    "# Inverse transform to original space from PCA space\n",
    "x_rotations = pca_x.inverse_transform(x_rotations.T).T  # Inverse transform x rotations\n",
    "y_rotations = pca_y.inverse_transform(y_rotations.T).T  # Inverse transform y rotations \n",
    "\n",
    "# Calculate scores\n",
    "x_score_ctrl = np.dot(x_ctrl_ss, x_rotations)  # Calculate x scores\n",
    "y_score_ctrl = np.dot(y_ctrl_ss, y_rotations)  # Calculate y scores\n",
    "\n",
    "# Calculate loadings as correlation coefficients between original data and scores\n",
    "x_loadings = np.zeros((np.shape(x_ctrl_ss)[1], ndim))\n",
    "y_loadings = np.zeros((np.shape(y_ctrl_ss)[1], ndim))\n",
    "for i in range(ndim):\n",
    "    x_loadings[:, i] = np.array([np.corrcoef(x_ctrl_ss[:, j], x_score_ctrl[:, i])[\n",
    "                                0, 1] for j in range(np.shape(x_ctrl_ss)[1])])\n",
    "    y_loadings[:, i] = np.array([np.corrcoef(y_ctrl_ss[:, j], y_score_ctrl[:, i])[\n",
    "                                0, 1] for j in range(np.shape(y_ctrl_ss)[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boostrap test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootrap loadings\n",
    "nboot = 1000  # Number of bootstrap iterations\n",
    "pls_boot = PLSCanonical(n_components=ndim+3, scale=False, max_iter=10000)  # Initialize PLS model for bootstrapping\n",
    "x_loadings_boot = []\n",
    "y_loadings_boot = []\n",
    "i = 0\n",
    "cct = 0\n",
    "ccq = 0\n",
    "while len(x_loadings_boot) < nboot:  # Continue until we have enough bootstraps\n",
    "    i = i + 1\n",
    "    # Draw bootstrap samples with replacement\n",
    "    x_boot_pca = resample(x_ctrl_pca, replace=True, n_samples=len(x_ctrl_ss),\n",
    "                          random_state=i)\n",
    "    y_boot_pca = resample(y_ctrl_pca, replace=True, n_samples=len(y_ctrl_pca),\n",
    "                          random_state=i)\n",
    "    pls_boot.fit(x_boot_pca, y_boot_pca)\n",
    "\n",
    "    x_rotations_boot = pca_x.inverse_transform(pls_boot.x_rotations_.T).T\n",
    "    y_rotations_boot = pca_y.inverse_transform(pls_boot.y_rotations_.T).T\n",
    "\n",
    "    # Calculate scores\n",
    "    x_score_boot = np.dot(x_ctrl_ss, x_rotations_boot)\n",
    "    y_score_boot = np.dot(y_ctrl_ss, y_rotations_boot)\n",
    "\n",
    "    # Loadings\n",
    "    tmp_x_loadings_boot = np.zeros((np.shape(x_ctrl_ss)[1], ndim+2))\n",
    "    tmp_y_loadings_boot = np.zeros((np.shape(y_ctrl_ss)[1], ndim+2))\n",
    "    for j in range(ndim+2):\n",
    "        tmp_x_loadings_boot[:, j] = np.array([np.corrcoef(x_ctrl_ss[:, k],\n",
    "                                                          x_score_boot[:, j])[0, 1] for k in range(np.shape(x_ctrl_ss)[1])])\n",
    "        tmp_y_loadings_boot[:, j] = np.array([np.corrcoef(y_ctrl_ss[:, k],\n",
    "                                                          y_score_boot[:, j])[0, 1] for k in range(np.shape(y_ctrl_ss)[1])])\n",
    "\n",
    "    # Check correspondence between PLS dimensions\n",
    "    # Find the best correspondence between the original and bootstrapped loadings\n",
    "    # by maximizing the correlation between them\n",
    "    order = []\n",
    "    flag = []\n",
    "    max_corr = []\n",
    "    for j in range(ndim):\n",
    "        tmp_corr = []\n",
    "        for k in range(ndim+2):\n",
    "             tmp_corr.append(np.corrcoef(x_loadings[:, j], tmp_x_loadings_boot[:, k])[0, 1])\n",
    "        idx = np.argmax(np.abs(tmp_corr))\n",
    "        order.append(idx)\n",
    "        flag.append(np.sign(tmp_corr[idx]))\n",
    "        max_corr.append(np.max(np.abs(tmp_corr)))\n",
    "    if np.unique(order).shape[0] != len(order):  # Check if all dimensions are unique\n",
    "        ccq = ccq + 1\n",
    "        continue\n",
    "    if np.min(max_corr) < 0.5:  # Check if the maximum correlation is above a threshold\n",
    "        cct = cct + 1\n",
    "        continue  \n",
    "    tmp_x_loadings_boot = tmp_x_loadings_boot[:, order]\n",
    "    tmp_y_loadings_boot = tmp_y_loadings_boot[:, order]\n",
    "    # Flip loadings\n",
    "    for j in range(ndim):\n",
    "        tmp_x_loadings_boot[:, j] = flag[j]*tmp_x_loadings_boot[:, j]\n",
    "        tmp_y_loadings_boot[:, j] = flag[j]*tmp_y_loadings_boot[:, j]\n",
    "\n",
    "    x_loadings_boot.append(tmp_x_loadings_boot)\n",
    "    y_loadings_boot.append(tmp_y_loadings_boot)\n",
    "x_loadings_boot = np.array(x_loadings_boot)\n",
    "y_loadings_boot = np.array(y_loadings_boot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boostrap significance test\n",
    "x_loadings_sig = []\n",
    "y_loadings_sig = []\n",
    "x_loadings_low = np.zeros((np.shape(x_ctrl_ss)[1], ndim))\n",
    "x_loadings_high = np.zeros((np.shape(x_ctrl_ss)[1], ndim))\n",
    "y_loadings_low = np.zeros((np.shape(y_ctrl_ss)[1], ndim))\n",
    "y_loadings_high = np.zeros((np.shape(y_ctrl_ss)[1], ndim))\n",
    "# Calculate the 95% confidence intervals for the loadings\n",
    "# and determine if the loadings are significantly different from zero\n",
    "for i in range(ndim):  \n",
    "    low, high = np.percentile(x_loadings_boot[:, :, i], [2.5, 97.5], axis=0)\n",
    "    x_loadings_sig.append(((np.sign(high) * np.sign(low)) > 0)*1)\n",
    "    x_loadings_low[:, i] = low\n",
    "    x_loadings_high[:, i] = high\n",
    "\n",
    "    low, high = np.percentile(y_loadings_boot[:, :, i], [2.5, 97.5], axis=0)\n",
    "    y_loadings_sig.append(((np.sign(high) * np.sign(low)) > 0)*1)\n",
    "    y_loadings_low[:, i] = low\n",
    "    y_loadings_high[:, i] = high\n",
    "\n",
    "x_loadings_sig = np.array(x_loadings_sig).T\n",
    "y_loadings_sig = np.array(y_loadings_sig).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PheWAS loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(ndim):\n",
    "    df_ph = pd.DataFrame(data=y_loadings[:, i], columns=['beta'], index=range(np.shape(y_loadings)[0]))\n",
    "    df_ph['phen_name'] = df_phens.phen_name\n",
    "    df_ph['phen_code'] = df_phens.instrument_x\n",
    "    df_ph['phen_cat'] = df_phens.phen_cat\n",
    "    df_ph['phen_cat_num'] = label_encoder.fit_transform(df_ph['phen_cat'])\n",
    "    df_ph['abs beta'] = np.abs(df_ph['beta'])\n",
    "\n",
    "    df_ph.reset_index(inplace=True, drop=False)\n",
    "\n",
    "    # Plot PheWAS\n",
    "    manhattan_plot(df_ph,...)\n",
    "    # Average phewas by category\n",
    "    df = df_ph.loc[:, ['phen_cat', 'abs beta']].groupby(['phen_cat']).mean()\n",
    "\n",
    "    radar_barplot(df['abs beta'].values, label=df.index,\n",
    "                 axis_range=(np.min(df['abs beta'].values)-0.01,\n",
    "                             np.max(df['abs beta'].values)+0.01))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the highest brain loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2 = '#73818C'\n",
    "c1 = '#9F5358'\n",
    "for i in range(ndim):\n",
    "    mean_reg = []\n",
    "    # Average loadings for left and right hemisphere\n",
    "    for j in range(int(len(region_names)/2)):\n",
    "        mean_reg.append(np.mean([x_loadings[j, i], x_loadings[j+74, i]]))\n",
    "    order = np.tile(np.argsort(-np.abs(mean_reg)), 2)\n",
    "\n",
    "    # Create dataframe for plotting\n",
    "    df = pd.DataFrame({'region': np.array(my_regions_hem)[order],\n",
    "                       'data': x_loadings[:, i],\n",
    "                       'data_abs': np.abs(x_loadings[:, i]),\n",
    "                       'hem': np.concatenate((np.repeat('L', 74),\n",
    "                                              np.repeat('R', 74)))})\n",
    "    \n",
    "    # Plot horizontal barplot with bootstrapped confidence intervals\n",
    "    fig, ax = plt.subplots(1, figsize=(4, 8))\n",
    "    for j in range(int(len(region_names)/2)):\n",
    "        plt.barh(0+j, height=0.3, width=x_loadings[order[j], i], color=c1)\n",
    "        plt.barh(0.4+j, height=0.3, width=x_loadings[order[j]+74, i], color=c2)\n",
    "        # Plot error\n",
    "        plt.plot([x_loadings_low[order[j], i],\n",
    "                  x_loadings_high[order[j], i]],\n",
    "                 [j, j], color=c1,)\n",
    "        plt.plot([x_loadings_low[order[j]+74, i],\n",
    "                  x_loadings_high[order[j]+74, i]],\n",
    "                 [j+0.4, j+0.4], color=c2)\n",
    "\n",
    "    ax.set_title('', fontsize=22)\n",
    "    ax.set_xlabel('Brain loading (95% CI)')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_yticks(np.arange(0.2, 74.2, 1))\n",
    "    ax.set_yticklabels(np.array(my_regions)[order[:74]])\n",
    "    plt.legend(['Left', 'Right'], loc='upper right')\n",
    "    plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "    plt.ylim((20.8, -0.2))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNV_ABCD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
